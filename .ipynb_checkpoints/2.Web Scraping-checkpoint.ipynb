{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Web Scraping</h1>\n",
    "<HR WIDTH=\"100%\" size=\"6\">\n",
    "\n",
    "\n",
    "<table align='left'>\n",
    "  <tr>\n",
    "    <td><b>Step</b></td>\n",
    "    <td><b>Description</b></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td><b>2.1 Function:</b>mongo_database_setup()</td>\n",
    "  <td>Connect to the mongo DB. Create new database based on date. Insert two collections (reports,comments)</td>\n",
    "  </tr>\n",
    "  <tr>  \n",
    "  <td><b>2.2 Function: </b>web_scrape(web_index)</td>\n",
    "  <td>Scrap webpage and return <b>BeautifulSoup</p> object</td>\n",
    "  </tr>\n",
    "  <tr>  \n",
    "  <td><b>2.3 Function: </b>parse_tables(soup,web_id,reports,comments)</td>\n",
    "  <td>Take first table in <b>BeautifulSoup</b> object, and loop through through the rows of the table. Extract the data using \n",
    "  regular expressions. Save to python dictionary and save dictionary to Mongo DB collection <i>reports</i>. Take third table ,loop through rows extract data with regular expressions and save to python dictionary and save dictionary to Mongo DB collection <i>comments</i> </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "  <td><b>2.4 </b>Scrape, Parse, Save</td>\n",
    "      <td>Provide counters for control starting page, and number of pages to scrape. Loop through and call functions to save to Mongo DB. Terminate when counter limits reached.</td>\n",
    "  </tr>\n",
    "    </table><br clear=\"left\"/>\n",
    "\n",
    "<table align='left'>\n",
    "   <tr>\n",
    "   <th colspan=\"4\"><p style=\"text-align: center;\">Packages Used</p></th>\n",
    "  </tr>\n",
    "  <tr style=\"background-color:azure\">\n",
    "    <td>Package</td>\n",
    "    <td>Pre-installed with Anaconda</td>\n",
    "    <td>Install instruction from command line</td>\n",
    "    <td>Documentation Link</td>\n",
    "    </tr>\n",
    "   <tr>\n",
    "    <td>time</td>\n",
    "    <td colspan=\"2\"> <p> Part of the Python Standard Library</p></td>\n",
    "    <td>https://docs.python.org/2/library/time.html</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "    <td>pymongo</td>\n",
    "    <td><p style=\"text-align: center;\">&#x2718;</td>\n",
    "    <td>pip install pymongo</td>\n",
    "    <td>http://api.mongodb.org/python/current/</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>BeautifulSoup</td>\n",
    "    <td><p style=\"text-align: center;\">&#x2718;</td>\n",
    "    <td>pip install beautifulsoup4</td>\n",
    "    <td>http://www.crummy.com/software/BeautifulSoup/bs4/doc/</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>urllib2</td>\n",
    "     <td colspan=\"2\"> <p> Part of the Python Standard Library (v2)</p></td>\n",
    "    <td>https://docs.python.org/2/library/urllib2.html</td>\n",
    "    <tr>\n",
    "    <td>re</td>\n",
    "    <td colspan=\"2\"> <p> Part of the Python Standard Library</p></td>\n",
    "    <td>https://docs.python.org/2/library/re.html</td>\n",
    "    </tr>\n",
    "   </table><br clear=\"left\"/>\n",
    "\n",
    "\n",
    "\n",
    "<HR WIDTH=\"100%\" size=\"4\">\n",
    "<font size=\"5\" color=\"red\">A instance of MongoDB must be running before executing this notebook</font>\n",
    "\n",
    "<HR WIDTH=\"100%\" size=\"6\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1 Connect and Setup MongoDB </h3>\n",
    "\n",
    "<p><font size=\"3\" color=\"red\">Mongo database must be running on local machine.</font> Create the connection to the mongo database. If successful connected then create a database <i>pillreport_ddmmyy</i>. If this database exists delete it and create again. Define two collections in the database <b>reports</b> and <b>comments</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pymongo\n",
    "\n",
    "def mongo_database_setup():\n",
    "    database_name={}\n",
    "\n",
    "    # Try to connect to MongoDB,  exit if not successful.\n",
    "    try:\n",
    "        conn=pymongo.MongoClient()\n",
    "        print \"Connected successfully!!!\"\n",
    "        \n",
    "    except pymongo.errors.ConnectionFailure, e:\n",
    "       print \"Could not connect to MongoDB: %s\" % e \n",
    "\n",
    "\n",
    "    #Use todays date for the database name:\n",
    "    name='pillreports_'+time.strftime(\"%d%b%y\")\n",
    "\n",
    "    if name in conn.database_names():\n",
    "        conn.drop_database(name) #Drop the database if it exists\n",
    "        db = conn[name] #Create the database\n",
    "        \n",
    "        #Create two collections in the database\n",
    "        reports = db.reports\n",
    "        comments=db.comments\n",
    "   \n",
    "    else:\n",
    "        db = conn[name] #Create the database\n",
    "        #Create two collections in the database\n",
    "        reports = db.reports\n",
    "        comments=db.comments\n",
    "   \n",
    "    #return the connection, database name, collections names.     \n",
    "    return conn,db,reports,comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2 Scrape individual reports and comments</h3>\n",
    "\n",
    "<p>Using the <b>urllib2</b> library; open and read a web page. Pass web page to <b>BeautifulSoup</b> library. Parse and extract all tables. If there are not 3 tables in the webpage return. Otherwise return the parsed webpage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "def web_scrape(web_index):\n",
    "    base_path=\"http://www.pillreports.net/index.php?page=display_pill&id=\"\n",
    "    web_path =base_path+str(web_index)\n",
    "    \n",
    "    #Open and read web page. \n",
    "    #Parse returned webpage with BeautifulSoup and extract all the tables in it. \n",
    "    try:\n",
    "        web_page = urllib2.urlopen(web_path).read().decode('utf-8')\n",
    "        soup=BeautifulSoup(web_page)\n",
    "        number_of_tables = len(soup.findChildren('table'))\n",
    "     \n",
    "    #Based on research if there are not 3 tables exit and return false. Indicates that the webpage is not published report.\n",
    "    #Report is in the first table. Comments are in the third table.\n",
    "        if number_of_tables!=3:\n",
    "            return False\n",
    "        else:\n",
    "            return soup\n",
    "\n",
    "    #If encoding cannot be determined exit. \n",
    "    except (UnicodeDecodeError):\n",
    "        print \"Encoding error encountered. Page \" +str(web_index)+ \" skipped.\"\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.3 Parse \"Description\" and \"Comments\" tables</h3>\n",
    "\n",
    "<p> Pass in the <b>Beutifulsoup</b> object,  webpage index, and pointers to the <i>reports</i> and <i>comments</i> collections in the MongoDB database. \n",
    "\n",
    "<p> Loop through rows in the first Table. Column 1 is table headers, column 2 is the data. Save to phyton dictionary and insert dictionary into <i>reports</i> collection on Mongo databse. Advantage of dictionary is do not have to specify keys in advance. <p>\n",
    "\n",
    "<p> Loop through third table containing the comments. Extract information using regular epxression to match strings. Save to python dictionary and insert each dictionary into the <i>comments</i> collection on Mongo database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_tables(soup,web_id,reports,comments):\n",
    "    \n",
    "    #Dictionary to hold values, ID value to the index.\n",
    "    pill_report_dict={}\n",
    "    pill_report_dict['ID: ']=unicode(web_id)\n",
    "    \n",
    "    #Select the 1st table and loop through each row of the table\n",
    "    #extracting first column and second column using the find_next() method of BeautifulSoup\n",
    "    for tr_tag in soup.find_all('table')[0]('tr'):\n",
    "        col1=tr_tag.find_next()\n",
    "        col2=col1.find_next()\n",
    "    \n",
    "        #If the type of is td, remove all <br> tags. \n",
    "        if col2.name=='td':   \n",
    "            \n",
    "            #\\s is white space. Remove newlines etc. replace with one space.\n",
    "            col2.text_tidy=re.sub(r'\\s\\s+', ' ', col2.text) \n",
    "            \n",
    "            #Remove any <xx> replace with ''. This is will remove any HTML tags remaining.\n",
    "            col2.text_tidy=re.sub(r'<.*?>','', col2.text_tidy) \n",
    "            \n",
    "            #Add the row to the dictionary. col1 is the column1 header, col2 is the content.\n",
    "            pill_report_dict[col1.text] = unicode(col2.text_tidy)\n",
    " \n",
    "    #Insert the dictionary into the Mongo Database.\n",
    "    reports.insert(pill_report_dict)\n",
    "    \n",
    "    \n",
    "#####################################################################\n",
    "#                         Parse Comments                            #\n",
    "#####################################################################\n",
    "\n",
    "    \n",
    "    #Dictionary to hold values. \n",
    "    comment_report_dict={}\n",
    "    \n",
    "    #Index to hold the comment number\n",
    "    i=0\n",
    "        \n",
    "    #Loop trhough all rows in the third table of soup object.\n",
    "    for tr_tag in soup.find_all('table')[2]('tr'):\n",
    "        \n",
    "        try:\n",
    "            row1=tr_tag.find_next()\n",
    "\n",
    "            if row1.text==\"There are no comments\":\n",
    "               comment_report_dict={}\n",
    "               return\n",
    "            else:\n",
    "                #Don't save the first row of the table\n",
    "                if row1.name=='td' and i > 0:\n",
    "                    \n",
    "                    comment_report_dict={}\n",
    "                    comment_report_dict[\"Report ID:  \"] = str(web_id)\n",
    "                    comment_report_dict[\"Comment Number: \"]=str(i)\n",
    "                    \n",
    "                    #Select everything betweenn 'Posted on ' and 'GMT'\n",
    "                    comment_report_dict[\"Posted On: \"] = re.search(r'Posted on (.*) GMT',row1.text).group(1)\n",
    "                    #Select everything betweenn 'GMT by ' and '('\n",
    "                    comment_report_dict[\"By: \"] =        re.search(r'GMT by (.*) \\(',row1.text).group(1)\n",
    "                    #Select everything betweenn '(' and ')'\n",
    "                    comment_report_dict[\"Member details: \"] = re.search(r'\\((.*)\\)',row1.text).group(1)\n",
    "\n",
    "                    #Remove any white space and HTML tags \n",
    "                    tidy_comment = re.sub(r'\\s\\s+', ' ', unicode(row1.text))\n",
    "                    p = re.compile(r'<.*?>')\n",
    "                    tidy_comment=p.sub('', tidy_comment)\n",
    "                   \n",
    "                    #Select everthing after ')'\n",
    "                    comment_report_dict[\"Comment: \"]= re.search(r'\\)(.*)',tidy_comment).group(1)\n",
    "\n",
    "                    #Insert dictionary in comemnts collection on pymongo\n",
    "                    comments.insert(comment_report_dict)\n",
    "\n",
    "            i=i+1\n",
    "        \n",
    "        except(AttributeError):\n",
    "            print \"Encoding error encountered. Not all comments captured on document:\" +str(web_id)\n",
    "            #return\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.4 Scrape, Parse and Save</h3>\n",
    "\n",
    "<p>Create a new Mongo database with two collections, with user defined function <i>web_scrape(index):</i></p>\n",
    "\n",
    "<p>Set parameters for the number of reports to download. Start indexing for download</p>\n",
    "\n",
    "<p>Pass <i>top_index</i> to <i>web_scarpe(index)</i> function. Parse the return <b>BeautifulSoup</b> object in the <i>parse_tables(soup,index,reports,comments)</i> function. </p>\n",
    "\n",
    "<p>Decrement the counters. When <i>target</i> to download is reached, terminate and close Mongo DB connection. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!!!\n",
      "A total of 51 reports were saved. A total of 313 comments were saved to the database.\n",
      "34051\n"
     ]
    }
   ],
   "source": [
    "#Setup the database, get reference to the connection\n",
    "#and the collections. \n",
    "conn,db,reports,comments=mongo_database_setup()\n",
    "\n",
    "#Scrape x number of pages starting at the most recent. \n",
    "\n",
    "target=5001\n",
    "top_index=34120\n",
    "download_count=0\n",
    "\n",
    "while download_count <= target:\n",
    "    \n",
    "    soup=web_scrape(top_index)\n",
    "         \n",
    "    if not isinstance(soup, bool):\n",
    "        #print top_index\n",
    "        parse_tables(soup,top_index,reports,comments)\n",
    "        download_count=download_count+1\n",
    "    \n",
    "    top_index=top_index-1\n",
    " \n",
    "print \"A total of \"+str(db.reports.count())+\" reports were saved. A total of \"+str(db.comments.count())+\" comments were saved to the database.\"   \n",
    "\n",
    "####Ensure connection is closed to save the database####\n",
    "\n",
    "conn.close()\n",
    "print top_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'local', u'pillreports_31Mar15']\n"
     ]
    }
   ],
   "source": [
    "#Check database is saved.\n",
    "conn=pymongo.MongoClient()\n",
    "print conn.database_names()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
